# Simple crawler

When youâ€™re attacking a custom web application or large site, knowing all of the files accessible on the web
server can be very handy. To this end, a crawler can discover as much of the web application as possible: configuration files, leftover development files, debugging scripts, and other security breadcrumbs can provide sensitive information or expose functionality that software developers did not intend. 
The only way to discover this content is to use a brute-forcing tool to hunt down common filenames and directories.

The basic functionality of it. It [gets as good as the wordlist is](bf_locations).